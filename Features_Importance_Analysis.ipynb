{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import commonutils\n",
    "from commonutils import ModelResults\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "#from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import warnings\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from commonutils import ModelsStore\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from commonfuncsforcli import *\n",
    "\n",
    "sys.path.append('./CLossLr')\n",
    "import customlosslr as clr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = './Results'\n",
    "selected_basisset = \"MINIX\"\n",
    "selected_functional = \"PBE\"\n",
    "REMOVEFLPSFROMTRAINING = False\n",
    "\n",
    "models = {}\n",
    "for dir in os.listdir(rootdir):\n",
    "    if os.path.isdir(os.path.join(rootdir, dir)):\n",
    "        if dir == selected_functional+\"_\"+selected_basisset:\n",
    "            for file in os.listdir(os.path.join(rootdir, dir)):\n",
    "                if file.endswith('.pkl'):\n",
    "                    print(f'Loading model from {os.path.join(rootdir, dir, file)}')\n",
    "                    with open(os.path.join(rootdir, dir, file), 'rb') as f:\n",
    "                        model = pickle.load(f)\n",
    "                        modelname = file.replace('.pkl', '')\n",
    "                        print(f'  Loaded model {modelname}')\n",
    "                        models[modelname] = model\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = {\"Te\": \"Kinetic_Energy\", \\\n",
    "         \"V_NN\": \"Nuclear_Repulsion\",\\\n",
    "         \"V_eN\": \"One_Electron_Energy - Kinetic_Energy\",\\\n",
    "         \"EX\": \"EX\",\\\n",
    "         \"E_J\": \"Two_Electron_Energy - EX - EC\",\\\n",
    "         \"DC\": \"Dispersion_correction\",\\\n",
    "         \"EC\": \"EC\"}\n",
    "SHIFTFT = \"DC\"\n",
    "\n",
    "IMPORTMODELTOSTART = False\n",
    "\n",
    "functionals = []\n",
    "basis_sets = []\n",
    "\n",
    "print(\"Selected functional: \", selected_functional)\n",
    "print(\" Selected basis set: \", selected_basisset)\n",
    "print(\"        Functionals: \", functionals)\n",
    "print(\"         Basis sets: \", basis_sets)\n",
    "\n",
    "featuresvalues_perset,\\\n",
    "fullsetnames, \\\n",
    "models_results, \\\n",
    "supersetnames = readdata(shiftusingFT=SHIFTFT, \\\n",
    "                selected_functionalin=selected_functional, \\\n",
    "                selected_basisin=selected_basisset, \\\n",
    "                equations=equations)\n",
    "\n",
    "sep = \"_\"\n",
    "for setname in fullsetnames:\n",
    "    desciptors = {}\n",
    "    k = selected_functional + sep +\\\n",
    "                selected_basisset \n",
    "    for features in featuresvalues_perset[setname]:\n",
    "        for val in features:\n",
    "            if val.find(k) != -1:\n",
    "                if val not in desciptors:\n",
    "                    desciptors[val] = [features[val]]\n",
    "                else:\n",
    "                    desciptors[val].append(features[val])\n",
    "\n",
    "    for features in featuresvalues_perset[setname]:\n",
    "        for val in features:\n",
    "            for func in functionals:\n",
    "                for basis in basis_sets:\n",
    "                    if not(basis == selected_basisset and \\\n",
    "                            func == selected_functional):\n",
    "                        if val.find(func + sep + basis) != -1:\n",
    "                            actualk = val \n",
    "                            refk  = selected_functional + sep  + \\\n",
    "                                    selected_basisset + \\\n",
    "                                    val.replace(func + sep + basis, \"\")\n",
    "                            newk = actualk + \"_difftoref\"\n",
    "                            if newk not in desciptors:\n",
    "                                desciptors[newk] = [features[actualk]-features[refk]]\n",
    "                            else:\n",
    "                                desciptors[newk].append(features[actualk]-features[refk])\n",
    "    \n",
    "    models_results[setname].features = desciptors\n",
    "\n",
    "# features selection\n",
    "setname = \"Full\"\n",
    "numoffeat = len(models_results[setname].features)\n",
    "print(\"Number of features for \", numoffeat)\n",
    "for setname in fullsetnames:\n",
    "    if len(models_results[setname].features) != numoffeat:\n",
    "        print(\"Number of features for \", setname, \" is different\")\n",
    "        sys.exit(1)\n",
    "\n",
    "toremove = []\n",
    "setname = \"Full\"\n",
    "for k in models_results[setname].features:\n",
    "    if len(set(models_results[setname].features[k])) == 1:\n",
    "        toremove.append(k)\n",
    "        print(\"Constant fatures to remove: \", k)\n",
    "\n",
    "# remove constant values\n",
    "for setname in fullsetnames:\n",
    "    for k in toremove:\n",
    "        del models_results[setname].features[k]\n",
    "\n",
    "# force removing features Nuclear Repulsion difference\n",
    "tormfeatname = set()\n",
    "print(\"Removing Nuclear Repulsion differences\")\n",
    "for setname in fullsetnames: \n",
    "    toremove = []\n",
    "    for k in models_results[setname].features:\n",
    "        if k.find(\"NR_difftoref\") != -1:\n",
    "            toremove.append(k)\n",
    "    for k in toremove:\n",
    "        #print(\"Removing feature \", k)\n",
    "        tormfeatname.add(k)\n",
    "        del models_results[setname].features[k]\n",
    "\n",
    "print(\"Removed features: \", tormfeatname)\n",
    "setname = \"Full\"\n",
    "numoffeat = len(models_results[setname].features)\n",
    "print(\"Number of features \", numoffeat)\n",
    "for setname in fullsetnames:\n",
    "    if len(models_results[setname].features) != numoffeat:\n",
    "        print(\"Number of features for \", setname, \" is different\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for setname in [\"Full\"] + list(supersetnames): \n",
    "    print(f'Processing dataset {setname}')\n",
    "    X, y, features_names = \\\n",
    "            commonutils.build_XY_matrix \\\n",
    "            (models_results[setname].features, \\\n",
    "            models_results[setname].labels)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test \\\n",
    "          = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    \n",
    "    for k in models:\n",
    "        if k.find(setname) != -1:\n",
    "            print(\"  Loading model:\", k)\n",
    "            model = models[k]\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f'  RMSE for dataset {setname}: {rmse}')\n",
    "            # run a permutation features importance analysis using permutation_importance\n",
    "            result = permutation_importance(model, X, y, \n",
    "                                            n_repeats=30, random_state=42,\n",
    "                                            scoring='neg_mean_squared_error')     \n",
    "            print(\"  Permutation feature importance:\")\n",
    "            for i in result.importances_mean.argsort()[::-1]:\n",
    "                print(f\"    {features_names[i]:20s}: {result.importances_mean[i]:.3e} \"\n",
    "                      f\"(std: {result.importances_std[i]:.3e})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "features = {}\n",
    "supersetnameslist = list(supersetnames.keys())\n",
    "for setname in featuresvalues_perset:\n",
    "    if setname in supersetnameslist:\n",
    "        print(\"Setname: \", setname)\n",
    "        for entry in featuresvalues_perset[setname]:\n",
    "            classes.append(supersetnameslist.index(setname))\n",
    "\n",
    "X, _, features_names =\\\n",
    "        commonutils.build_XY_matrix (\\\n",
    "        models_results['Full'].features,\\\n",
    "        models_results['Full'].labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "        X, classes, test_size=0.20, random_state=41)\n",
    "\n",
    "rfmodel = models[\"rfmodel\"]\n",
    "result = permutation_importance(rfmodel, X, classes, \n",
    "                        n_repeats=30, random_state=42,\n",
    "                        scoring='accuracy')     \n",
    "print(\"  Permutation feature importance:\")\n",
    "for i in result.importances_mean.argsort()[::-1]:\n",
    "        print(f\"    {features_names[i]:20s}: {result.importances_mean[i]:.3e} \"\n",
    "                f\"(std: {result.importances_std[i]:.3e})\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310andscik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
